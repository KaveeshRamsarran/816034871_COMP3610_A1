{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcfce3d9",
   "metadata": {},
   "source": [
    "# COMP 3610: Big Data Analytics - Assignment 1\n",
    "## Data Pipeline & Visualization Dashboard\n",
    "\n",
    "**Student ID:** 816034871\n",
    "\n",
    "**Dataset:** NYC Yellow Taxi Trip Data (January 2024)\n",
    "\n",
    "This notebook demonstrates the end-to-end data pipeline including:\n",
    "- Part 1: Data Ingestion & Storage\n",
    "- Part 2: Data Transformation & Analysis\n",
    "- Part 3: Visualization Prototypes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0068c323",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e395c699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install pandas pyarrow duckdb requests plotly streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a559f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"DuckDB version: {duckdb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817a03a2",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Data Ingestion & Storage (20 marks)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d4b52c",
   "metadata": {},
   "source": [
    "## 1.1 Programmatic Download (5 marks)\n",
    "\n",
    "Download the NYC Yellow Taxi Trip data and Taxi Zone Lookup table programmatically using Python's `requests` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61beac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data URLs\n",
    "TAXI_DATA_URL = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\"\n",
    "ZONE_LOOKUP_URL = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\"\n",
    "\n",
    "# Define data directory structure\n",
    "DATA_DIR = Path(\"data\")\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Data directories created:\")\n",
    "print(f\"  - Raw data: {RAW_DIR.absolute()}\")\n",
    "print(f\"  - Processed data: {PROCESSED_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f662242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url: str, destination: Path, chunk_size: int = 8192) -> bool:\n",
    "    \"\"\"\n",
    "    Download a file from a URL with progress tracking.\n",
    "    \n",
    "    Args:\n",
    "        url: The URL to download from\n",
    "        destination: The local file path to save to\n",
    "        chunk_size: Size of chunks to download at a time\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if download successful, raises exception otherwise\n",
    "    \"\"\"\n",
    "    print(f\"Downloading: {url}\")\n",
    "    print(f\"Destination: {destination}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Raise exception for HTTP errors\n",
    "        \n",
    "        # Get file size if available\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        \n",
    "        downloaded = 0\n",
    "        with open(destination, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=chunk_size):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    downloaded += len(chunk)\n",
    "                    if total_size > 0:\n",
    "                        progress = (downloaded / total_size) * 100\n",
    "                        print(f\"\\rProgress: {progress:.1f}% ({downloaded / 1024 / 1024:.1f} MB)\", end=\"\")\n",
    "        \n",
    "        print(f\"\\nDownload complete! File size: {destination.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "        return True\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise Exception(f\"Failed to download {url}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9be041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "taxi_data_path = RAW_DIR / \"yellow_tripdata_2024-01.parquet\"\n",
    "zone_lookup_path = RAW_DIR / \"taxi_zone_lookup.csv\"\n",
    "\n",
    "# Download taxi trip data (if not already downloaded)\n",
    "if not taxi_data_path.exists():\n",
    "    download_file(TAXI_DATA_URL, taxi_data_path)\n",
    "else:\n",
    "    print(f\"Taxi data already exists: {taxi_data_path}\")\n",
    "    print(f\"File size: {taxi_data_path.stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b0020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download zone lookup table (if not already downloaded)\n",
    "if not zone_lookup_path.exists():\n",
    "    download_file(ZONE_LOOKUP_URL, zone_lookup_path)\n",
    "else:\n",
    "    print(f\"Zone lookup already exists: {zone_lookup_path}\")\n",
    "    print(f\"File size: {zone_lookup_path.stat().st_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7122be5d",
   "metadata": {},
   "source": [
    "## 1.2 Data Validation (10 marks)\n",
    "\n",
    "Validate the downloaded data by:\n",
    "- Verifying expected columns exist\n",
    "- Checking datetime types\n",
    "- Reporting row counts\n",
    "- Raising exceptions on validation failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c2a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define expected columns per assignment specification (Dataset Schema)\n",
    "EXPECTED_COLUMNS = [\n",
    "    'tpep_pickup_datetime',\n",
    "    'tpep_dropoff_datetime',\n",
    "    'PULocationID',\n",
    "    'DOLocationID',\n",
    "    'passenger_count',\n",
    "    'trip_distance',\n",
    "    'fare_amount',\n",
    "    'tip_amount',\n",
    "    'total_amount',\n",
    "    'payment_type'\n",
    "]\n",
    "\n",
    "DATETIME_COLUMNS = ['tpep_pickup_datetime', 'tpep_dropoff_datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c1a2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_taxi_data(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Validate the taxi trip data and return validation results.\n",
    "    \n",
    "    Args:\n",
    "        df: The taxi trip DataFrame to validate\n",
    "    \n",
    "    Returns:\n",
    "        dict: Validation results including status and details\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If validation fails\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'row_count': len(df),\n",
    "        'column_count': len(df.columns),\n",
    "        'columns_present': list(df.columns),\n",
    "        'missing_columns': [],\n",
    "        'datetime_valid': True,\n",
    "        'validation_passed': True,\n",
    "        'errors': []\n",
    "    }\n",
    "    \n",
    "    # Check for expected columns\n",
    "    for col in EXPECTED_COLUMNS:\n",
    "        if col not in df.columns:\n",
    "            results['missing_columns'].append(col)\n",
    "    \n",
    "    if results['missing_columns']:\n",
    "        results['validation_passed'] = False\n",
    "        results['errors'].append(f\"Missing columns: {results['missing_columns']}\")\n",
    "    \n",
    "    # Check datetime columns\n",
    "    for col in DATETIME_COLUMNS:\n",
    "        if col in df.columns:\n",
    "            if not pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "                results['datetime_valid'] = False\n",
    "                results['errors'].append(f\"Column {col} is not datetime type\")\n",
    "    \n",
    "    # Check row count is reasonable (should be > 0)\n",
    "    if results['row_count'] == 0:\n",
    "        results['validation_passed'] = False\n",
    "        results['errors'].append(\"DataFrame is empty\")\n",
    "    \n",
    "    if not results['validation_passed']:\n",
    "        raise ValueError(f\"Data validation failed: {results['errors']}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3465f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and validate the taxi trip data\n",
    "print(\"Loading taxi trip data...\")\n",
    "df_taxi = pd.read_parquet(taxi_data_path)\n",
    "\n",
    "print(\"\\nValidating data...\")\n",
    "validation_results = validate_taxi_data(df_taxi)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VALIDATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\u2713 Total rows: {validation_results['row_count']:,}\")\n",
    "print(f\"\u2713 Total columns: {validation_results['column_count']}\")\n",
    "print(f\"\u2713 Datetime columns valid: {validation_results['datetime_valid']}\")\n",
    "print(f\"\u2713 Validation passed: {validation_results['validation_passed']}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c032a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load zone lookup data\n",
    "print(\"Loading zone lookup data...\")\n",
    "df_zones = pd.read_csv(zone_lookup_path)\n",
    "\n",
    "print(f\"\\nZone Lookup Table:\")\n",
    "print(f\"  Rows: {len(df_zones):,}\")\n",
    "print(f\"  Columns: {list(df_zones.columns)}\")\n",
    "df_zones.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b738227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data info\n",
    "print(\"\\nTaxi Data Info:\")\n",
    "print(df_taxi.info())\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df_taxi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b8565a",
   "metadata": {},
   "source": [
    "## 1.3 File Organization (5 marks)\n",
    "\n",
    "Data files are saved to `data/raw/` directory. The `.gitignore` file excludes the data directory from version control to avoid committing large data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dad78e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display file organization\n",
    "print(\"Project File Organization:\")\n",
    "print(\"\")\n",
    "print(\"assignment1/\")\n",
    "print(\"\u251c\u2500\u2500 data/\")\n",
    "print(\"\u2502   \u251c\u2500\u2500 raw/\")\n",
    "print(\"\u2502   \u2502   \u251c\u2500\u2500 yellow_tripdata_2024-01.parquet\")\n",
    "print(\"\u2502   \u2502   \u2514\u2500\u2500 taxi_zone_lookup.csv\")\n",
    "print(\"\u2502   \u2514\u2500\u2500 processed/\")\n",
    "print(\"\u251c\u2500\u2500 assignment1.ipynb\")\n",
    "print(\"\u251c\u2500\u2500 app.py\")\n",
    "print(\"\u251c\u2500\u2500 requirements.txt\")\n",
    "print(\"\u251c\u2500\u2500 README.md\")\n",
    "print(\"\u2514\u2500\u2500 .gitignore  (excludes data/ directory)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa46ca3",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Data Transformation & Analysis (30 marks)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff6799",
   "metadata": {},
   "source": [
    "## 2.1 Data Cleaning (10 marks)\n",
    "\n",
    "Clean the data by:\n",
    "- Removing null values\n",
    "- Removing invalid trips (zero/negative distance, fare, or duration)\n",
    "- Documenting all removals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5190d5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_taxi_data(df: pd.DataFrame) -> tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Clean the taxi trip data and document removals.\n",
    "    \n",
    "    Args:\n",
    "        df: Raw taxi trip DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (cleaned DataFrame, cleaning statistics dict)\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If cleaning removes all rows\n",
    "    \"\"\"\n",
    "    original_count = len(df)\n",
    "    cleaning_stats = {\n",
    "        'original_rows': original_count,\n",
    "        'removals': {}\n",
    "    }\n",
    "    \n",
    "    # Make a copy to avoid modifying original\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # 1. Remove rows with null values in critical columns\n",
    "    critical_columns = ['tpep_pickup_datetime', 'tpep_dropoff_datetime', \n",
    "                        'trip_distance', 'fare_amount', 'total_amount',\n",
    "                        'PULocationID', 'DOLocationID']\n",
    "    \n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean.dropna(subset=critical_columns)\n",
    "    cleaning_stats['removals']['null_values'] = before - len(df_clean)\n",
    "    \n",
    "    # 2. Remove trips with zero or negative distance\n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean[df_clean['trip_distance'] > 0]\n",
    "    cleaning_stats['removals']['invalid_distance'] = before - len(df_clean)\n",
    "    \n",
    "    # 3. Remove trips with zero or negative fare\n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean[df_clean['fare_amount'] > 0]\n",
    "    cleaning_stats['removals']['invalid_fare'] = before - len(df_clean)\n",
    "    \n",
    "    # 4. Remove trips with fares exceeding $500\n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean[df_clean['fare_amount'] <= 500]\n",
    "    cleaning_stats['removals']['fare_over_500'] = before - len(df_clean)\n",
    "    \n",
    "    # 5. Remove trips with invalid total amount\n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean[df_clean['total_amount'] > 0]\n",
    "    cleaning_stats['removals']['invalid_total'] = before - len(df_clean)\n",
    "    \n",
    "    # 6. Remove trips where dropoff is before pickup\n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean[df_clean['tpep_dropoff_datetime'] > df_clean['tpep_pickup_datetime']]\n",
    "    cleaning_stats['removals']['dropoff_before_pickup'] = before - len(df_clean)\n",
    "    \n",
    "    # 7. Calculate duration and remove trips with invalid duration\n",
    "    df_clean['duration_seconds'] = (df_clean['tpep_dropoff_datetime'] - \n",
    "                                     df_clean['tpep_pickup_datetime']).dt.total_seconds()\n",
    "    \n",
    "    before = len(df_clean)\n",
    "    # Remove trips shorter than 1 minute or longer than 24 hours\n",
    "    df_clean = df_clean[(df_clean['duration_seconds'] >= 60) & \n",
    "                        (df_clean['duration_seconds'] <= 86400)]\n",
    "    cleaning_stats['removals']['invalid_duration'] = before - len(df_clean)\n",
    "    \n",
    "    # 8. Remove trips with unrealistic speeds (> 100 mph average)\n",
    "    df_clean['avg_speed_mph'] = df_clean['trip_distance'] / (df_clean['duration_seconds'] / 3600)\n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean[df_clean['avg_speed_mph'] <= 100]\n",
    "    cleaning_stats['removals']['unrealistic_speed'] = before - len(df_clean)\n",
    "    \n",
    "    # Drop temporary columns\n",
    "    df_clean = df_clean.drop(columns=['duration_seconds', 'avg_speed_mph'])\n",
    "    \n",
    "    # Calculate final statistics\n",
    "    cleaning_stats['final_rows'] = len(df_clean)\n",
    "    cleaning_stats['total_removed'] = original_count - len(df_clean)\n",
    "    cleaning_stats['removal_percentage'] = (cleaning_stats['total_removed'] / original_count) * 100\n",
    "    \n",
    "    return df_clean, cleaning_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad8e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "print(\"Cleaning taxi trip data...\")\n",
    "df_clean, cleaning_stats = clean_taxi_data(df_taxi)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATA CLEANING REPORT\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Original rows: {cleaning_stats['original_rows']:,}\")\n",
    "print(f\"\\nRows removed by reason:\")\n",
    "for reason, count in cleaning_stats['removals'].items():\n",
    "    print(f\"  - {reason}: {count:,}\")\n",
    "print(f\"\\nTotal rows removed: {cleaning_stats['total_removed']:,} ({cleaning_stats['removal_percentage']:.2f}%)\")\n",
    "print(f\"Final rows: {cleaning_stats['final_rows']:,}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ec2d3d",
   "metadata": {},
   "source": [
    "## 2.2 Feature Engineering (10 marks)\n",
    "\n",
    "Create exactly 4 new derived columns:\n",
    "1. `trip_duration_minutes` - Calculated from pickup and dropoff timestamps\n",
    "2. `trip_speed_mph` - Distance divided by duration (handle division by zero)\n",
    "3. `pickup_hour` - Hour of day (0-23) extracted from pickup timestamp\n",
    "4. `pickup_day_of_week` - Day name (Monday-Sunday) extracted from pickup timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead99f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add engineered features to the taxi trip data.\n",
    "    \n",
    "    Args:\n",
    "        df: Cleaned taxi trip DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with new feature columns\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Trip duration in minutes\n",
    "    df['trip_duration_minutes'] = (\n",
    "        (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime'])\n",
    "        .dt.total_seconds() / 60\n",
    "    ).round(2)\n",
    "    \n",
    "    # 2. Average trip speed in MPH (handle division by zero)\n",
    "    df['trip_speed_mph'] = (\n",
    "        df['trip_distance'] / (df['trip_duration_minutes'] / 60)\n",
    "    ).round(2)\n",
    "    \n",
    "    # 3. Pickup hour (0-23)\n",
    "    df['pickup_hour'] = df['tpep_pickup_datetime'].dt.hour\n",
    "    \n",
    "    # 4. Pickup day of week (day name: Monday-Sunday)\n",
    "    df['pickup_day_of_week'] = df['tpep_pickup_datetime'].dt.day_name()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0875001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add features to the cleaned data\n",
    "print(\"Adding engineered features...\")\n",
    "df_features = add_features(df_clean)\n",
    "\n",
    "print(\"\\nNew columns added:\")\n",
    "new_cols = ['trip_duration_minutes', 'trip_speed_mph', 'pickup_hour', 'pickup_day_of_week']\n",
    "for col in new_cols:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(\"\\nSample of new features:\")\n",
    "df_features[['tpep_pickup_datetime', 'trip_distance'] + new_cols].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cb3c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for new features\n",
    "print(\"\\nFeature Statistics:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nTrip Duration (minutes):\")\n",
    "print(f\"  Mean: {df_features['trip_duration_minutes'].mean():.2f}\")\n",
    "print(f\"  Median: {df_features['trip_duration_minutes'].median():.2f}\")\n",
    "print(f\"  Min: {df_features['trip_duration_minutes'].min():.2f}\")\n",
    "print(f\"  Max: {df_features['trip_duration_minutes'].max():.2f}\")\n",
    "\n",
    "print(f\"\\nTrip Speed (MPH):\")\n",
    "print(f\"  Mean: {df_features['trip_speed_mph'].mean():.2f}\")\n",
    "print(f\"  Median: {df_features['trip_speed_mph'].median():.2f}\")\n",
    "\n",
    "print(f\"\\nTrips by Day of Week:\")\n",
    "print(df_features['pickup_day_of_week'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6115316e",
   "metadata": {},
   "source": [
    "## 2.3 SQL Analysis with DuckDB (10 marks)\n",
    "\n",
    "Perform the following SQL queries using DuckDB:\n",
    "1. Top 10 busiest pickup zones\n",
    "2. Average fare by hour of day\n",
    "3. Payment type distribution (percentages)\n",
    "4. Average tip percentage by day of week (credit card only)\n",
    "5. Top 5 pickup-dropoff zone pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e3224d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DuckDB connection and register DataFrames\n",
    "con = duckdb.connect()\n",
    "con.register('trips', df_features)\n",
    "con.register('zones', df_zones)\n",
    "\n",
    "print(\"DuckDB connection established and DataFrames registered.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b56d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 1: Top 10 busiest pickup zones\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUERY 1: Top 10 Busiest Pickup Zones\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "query1 = \"\"\"\n",
    "SELECT \n",
    "    z.Zone as pickup_zone,\n",
    "    z.Borough as borough,\n",
    "    COUNT(*) as trip_count,\n",
    "    ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM trips), 2) as percentage\n",
    "FROM trips t\n",
    "JOIN zones z ON t.PULocationID = z.LocationID\n",
    "GROUP BY z.Zone, z.Borough\n",
    "ORDER BY trip_count DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "df_query1 = con.execute(query1).fetchdf()\n",
    "print(df_query1.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3c53b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 2: Average fare by hour of day\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUERY 2: Average Fare by Hour of Day\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "query2 = \"\"\"\n",
    "SELECT \n",
    "    pickup_hour,\n",
    "    ROUND(AVG(fare_amount), 2) as avg_fare,\n",
    "    ROUND(AVG(total_amount), 2) as avg_total,\n",
    "    COUNT(*) as trip_count\n",
    "FROM trips\n",
    "GROUP BY pickup_hour\n",
    "ORDER BY pickup_hour\n",
    "\"\"\"\n",
    "\n",
    "df_query2 = con.execute(query2).fetchdf()\n",
    "print(df_query2.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79218cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 3: Payment type distribution (percentages)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUERY 3: Payment Type Distribution\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "query3 = \"\"\"\n",
    "SELECT \n",
    "    CASE payment_type\n",
    "        WHEN 1 THEN 'Credit Card'\n",
    "        WHEN 2 THEN 'Cash'\n",
    "        WHEN 3 THEN 'No Charge'\n",
    "        WHEN 4 THEN 'Dispute'\n",
    "        WHEN 5 THEN 'Unknown'\n",
    "        WHEN 6 THEN 'Voided Trip'\n",
    "        ELSE 'Other'\n",
    "    END as payment_method,\n",
    "    COUNT(*) as trip_count,\n",
    "    ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM trips), 2) as percentage\n",
    "FROM trips\n",
    "GROUP BY payment_type\n",
    "ORDER BY trip_count DESC\n",
    "\"\"\"\n",
    "\n",
    "df_query3 = con.execute(query3).fetchdf()\n",
    "print(df_query3.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3622ead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 4: Average tip percentage by day of week (credit card only)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUERY 4: Average Tip Percentage by Day (Credit Card Only)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "query4 = \"\"\"\n",
    "SELECT \n",
    "    pickup_day_of_week as day_of_week,\n",
    "    COUNT(*) as trip_count,\n",
    "    ROUND(AVG(tip_amount), 2) as avg_tip,\n",
    "    ROUND(AVG(tip_amount / NULLIF(fare_amount, 0) * 100), 2) as avg_tip_percentage\n",
    "FROM trips\n",
    "WHERE payment_type = 1  -- Credit card only\n",
    "    AND fare_amount > 0\n",
    "GROUP BY pickup_day_of_week\n",
    "ORDER BY CASE pickup_day_of_week\n",
    "    WHEN 'Monday' THEN 1\n",
    "    WHEN 'Tuesday' THEN 2\n",
    "    WHEN 'Wednesday' THEN 3\n",
    "    WHEN 'Thursday' THEN 4\n",
    "    WHEN 'Friday' THEN 5\n",
    "    WHEN 'Saturday' THEN 6\n",
    "    WHEN 'Sunday' THEN 7\n",
    "END\n",
    "\"\"\"\n",
    "\n",
    "df_query4 = con.execute(query4).fetchdf()\n",
    "print(df_query4.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13754e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 5: Top 5 pickup-dropoff zone pairs\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUERY 5: Top 5 Pickup-Dropoff Zone Pairs\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "query5 = \"\"\"\n",
    "SELECT \n",
    "    pz.Zone as pickup_zone,\n",
    "    dz.Zone as dropoff_zone,\n",
    "    COUNT(*) as trip_count,\n",
    "    ROUND(AVG(t.trip_distance), 2) as avg_distance,\n",
    "    ROUND(AVG(t.total_amount), 2) as avg_total\n",
    "FROM trips t\n",
    "JOIN zones pz ON t.PULocationID = pz.LocationID\n",
    "JOIN zones dz ON t.DOLocationID = dz.LocationID\n",
    "GROUP BY pz.Zone, dz.Zone\n",
    "ORDER BY trip_count DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "df_query5 = con.execute(query5).fetchdf()\n",
    "print(df_query5.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38031569",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Visualization Prototypes (40 marks)\n",
    "---\n",
    "\n",
    "Prototype visualizations for the Streamlit dashboard using Plotly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912e4f0a",
   "metadata": {},
   "source": [
    "## 3.1 Key Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4757df99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate key metrics\n",
    "metrics = {\n",
    "    'Total Trips': f\"{len(df_features):,}\",\n",
    "    'Average Fare': f\"${df_features['fare_amount'].mean():.2f}\",\n",
    "    'Total Revenue': f\"${df_features['total_amount'].sum():,.2f}\",\n",
    "    'Average Distance': f\"{df_features['trip_distance'].mean():.2f} miles\",\n",
    "    'Average Duration': f\"{df_features['trip_duration_minutes'].mean():.1f} mins\"\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY METRICS\")\n",
    "print(\"=\"*60)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd94c4d",
   "metadata": {},
   "source": [
    "## 3.2 Visualization 1: Top 10 Pickup Zones (Bar Chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4544b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart: Top 10 pickup zones\n",
    "fig1 = px.bar(\n",
    "    df_query1,\n",
    "    x='pickup_zone',\n",
    "    y='trip_count',\n",
    "    color='borough',\n",
    "    title='Top 10 Busiest Pickup Zones',\n",
    "    labels={'pickup_zone': 'Pickup Zone', 'trip_count': 'Number of Trips', 'borough': 'Borough'},\n",
    "    text='trip_count'\n",
    ")\n",
    "fig1.update_traces(texttemplate='%{text:,.0f}', textposition='outside')\n",
    "fig1.update_layout(xaxis_tickangle=-45, height=500)\n",
    "fig1.show()\n",
    "\n",
    "print(\"\\nInsight: The busiest pickup zones are concentrated in Manhattan, with areas like \")\n",
    "print(\"Upper East Side, Midtown, and Penn Station seeing the highest taxi demand.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772c2ac4",
   "metadata": {},
   "source": [
    "## 3.3 Visualization 2: Average Fare by Hour (Line Chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce7d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line chart: Average fare by hour\n",
    "fig2 = px.line(\n",
    "    df_query2,\n",
    "    x='pickup_hour',\n",
    "    y='avg_fare',\n",
    "    title='Average Fare by Hour of Day',\n",
    "    labels={'pickup_hour': 'Hour of Day', 'avg_fare': 'Average Fare ($)'},\n",
    "    markers=True\n",
    ")\n",
    "fig2.update_layout(\n",
    "    xaxis=dict(tickmode='linear', dtick=1),\n",
    "    height=400\n",
    ")\n",
    "fig2.show()\n",
    "\n",
    "print(\"\\nInsight: Average fares peak during early morning hours (4-6 AM), likely due to \")\n",
    "print(\"airport trips and longer distances traveled during off-peak hours.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f51e2a6",
   "metadata": {},
   "source": [
    "## 3.4 Visualization 3: Trip Distance Distribution (Histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86f278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram: Trip distance distribution\n",
    "fig3 = px.histogram(\n",
    "    df_features[df_features['trip_distance'] <= 30],  # Filter outliers\n",
    "    x='trip_distance',\n",
    "    nbins=50,\n",
    "    title='Distribution of Trip Distances',\n",
    "    labels={'trip_distance': 'Trip Distance (miles)', 'count': 'Number of Trips'}\n",
    ")\n",
    "fig3.update_layout(height=400)\n",
    "fig3.show()\n",
    "\n",
    "print(\"\\nInsight: Most taxi trips are short-distance, with the majority under 5 miles.\")\n",
    "print(\"The right-skewed distribution shows typical urban taxi usage patterns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bbc83d",
   "metadata": {},
   "source": [
    "## 3.5 Visualization 4: Payment Type Distribution (Pie Chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3ace06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart: Payment type distribution\n",
    "fig4 = px.pie(\n",
    "    df_query3,\n",
    "    values='trip_count',\n",
    "    names='payment_method',\n",
    "    title='Payment Type Distribution',\n",
    "    hole=0.4  # Donut chart\n",
    ")\n",
    "fig4.update_traces(textposition='inside', textinfo='percent+label')\n",
    "fig4.update_layout(height=450)\n",
    "fig4.show()\n",
    "\n",
    "print(\"\\nInsight: Credit cards are the dominant payment method, accounting for the vast \")\n",
    "print(\"majority of transactions, reflecting modern payment preferences in NYC taxis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d13676",
   "metadata": {},
   "source": [
    "## 3.6 Visualization 5: Trips Heatmap by Day and Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c094346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap data\n",
    "day_order_map = {'Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3, \n",
    "                 'Friday': 4, 'Saturday': 5, 'Sunday': 6}\n",
    "df_features['day_num'] = df_features['pickup_day_of_week'].map(day_order_map)\n",
    "heatmap_data = df_features.groupby(['day_num', 'pickup_hour']).size().reset_index(name='trips')\n",
    "heatmap_pivot = heatmap_data.pivot(index='day_num', columns='pickup_hour', values='trips')\n",
    "\n",
    "# Day labels\n",
    "day_labels = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "fig5 = px.imshow(\n",
    "    heatmap_pivot,\n",
    "    labels=dict(x='Hour of Day', y='Day of Week', color='Trip Count'),\n",
    "    x=list(range(24)),\n",
    "    y=day_labels,\n",
    "    title='Trip Volume Heatmap: Day of Week vs Hour',\n",
    "    color_continuous_scale='YlOrRd',\n",
    "    aspect='auto'\n",
    ")\n",
    "fig5.update_layout(height=400)\n",
    "fig5.show()\n",
    "\n",
    "print(\"\\nInsight: Peak taxi usage occurs on weekday evenings (5-7 PM) and Friday/Saturday \")\n",
    "print(\"nights. Early morning hours (2-5 AM) show the lowest demand across all days.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d372a563",
   "metadata": {},
   "source": [
    "## 3.7 Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91dd778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data for the Streamlit app\n",
    "processed_path = PROCESSED_DIR / \"taxi_data_processed.parquet\"\n",
    "df_features.to_parquet(processed_path, index=False)\n",
    "print(f\"Processed data saved to: {processed_path}\")\n",
    "print(f\"File size: {processed_path.stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de245a32",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "---\n",
    "\n",
    "This notebook demonstrates a complete data pipeline for the NYC Yellow Taxi Trip dataset:\n",
    "\n",
    "**Part 1 - Data Ingestion:**\n",
    "- Downloaded ~3 million trip records programmatically\n",
    "- Validated data structure and types\n",
    "- Organized files in proper directory structure\n",
    "\n",
    "**Part 2 - Data Transformation:**\n",
    "- Cleaned data by removing nulls and invalid records\n",
    "- Created derived features (duration, speed, temporal features)\n",
    "- Performed SQL analysis using DuckDB\n",
    "\n",
    "**Part 3 - Visualizations:**\n",
    "- Prototyped 5 interactive visualizations using Plotly\n",
    "- Calculated key business metrics\n",
    "- Prepared data for Streamlit dashboard\n",
    "\n",
    "The processed data is ready for deployment in the Streamlit dashboard (`app.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd667f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close DuckDB connection\n",
    "con.close()\n",
    "print(\"Analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ai-tools-disclosure",
   "metadata": {},
   "source": [
    "---\n",
    "# AI Tools Used\n",
    "---\n",
    "\n",
    "This assignment was completed with assistance from **GitHub Copilot** powered by Claude (Anthropic).\n",
    "\n",
    "## How AI Was Used\n",
    "\n",
    "### 1. Code Generation & Structure\n",
    "- Generated boilerplate code for data ingestion pipeline\n",
    "- Suggested function structures for data cleaning and validation\n",
    "- Provided code patterns for DuckDB SQL integration\n",
    "\n",
    "### 2. Data Transformation\n",
    "- Assisted with Pandas operations for data cleaning\n",
    "- Suggested approaches for feature engineering (trip duration, speed calculations)\n",
    "- Helped structure temporal feature extraction\n",
    "\n",
    "### 3. SQL Queries\n",
    "- Generated DuckDB SQL query syntax\n",
    "- Suggested aggregation patterns for business analytics\n",
    "- Optimized query structure for performance\n",
    "\n",
    "### 4. Visualizations\n",
    "- Provided Plotly Express code patterns\n",
    "- Suggested chart customizations and color schemes\n",
    "- Helped structure the visualization layout\n",
    "\n",
    "### 5. Documentation & Code Quality\n",
    "- Generated docstrings and comments\n",
    "- Suggested README structure and content\n",
    "- Assisted with requirements.txt dependencies\n",
    "\n",
    "## Verification\n",
    "All AI-generated code was reviewed, tested, and modified as needed to ensure correctness and alignment with assignment requirements."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}